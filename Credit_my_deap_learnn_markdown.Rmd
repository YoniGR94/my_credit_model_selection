---
title: "Different Techniches to Analyze Credit Balance"
output: github_document
always_allow_html: true
---

How good can we predict Credit balance?

What is the best way to check it?

In order to race our models, we will use the Credit data from ILSR package.

About this data:

"A simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt."* 

<font size="2">*[RDocumentation](https://www.rdocumentation.org/packages/ISLR/versions/1.2/topics/Credit)</font>

# prologue

## set Data

first, let's set the Environment

libraries:

```{r library, error=FALSE, message=FALSE, warning=FALSE}
library(easypackages)
library(tidymodels)
#collect data:
library(ISLR)
attach(Credit)
#random Forest
library(randomForest)
#other models
libraries('glmnet','Rcmdr','MASS')
#neural network
libraries('tensorflow','keras')
#install_tensorflow() #install_keras() #install_minicomda
#data tools:
libraries('dplyr','tidyr')
libraries('Matrix','scales')
#visual
libraries('hrbrthemes','viridis')
libraries('knitr','kableExtra','sjPlot')
```

Now we will see the data's structure

6 top rows of our table:

```{r setup, include = FALSE}
#knitr::opts_chunk$set(cache = TRUE)
```

```{r see data 6, warning=FALSE, message=FALSE,cache=FALSE}
head(Credit[,-1],6) %>%
  kbl() %>%
  kable_material(c("striped", "hover"))
```

Here a histogram of he credit balance:

```{r histogram Balance, error=FALSE, message=FALSE, warning=FALSE ,cache=FALSE}
Credit %>%
  ggplot(aes(x=Balance))+
  geom_histogram(color= alpha("black", 0.7),fill= alpha("green", 0.8) )+
  geom_vline(xintercept= mean(Balance), color= "red",lty= "dashed")+
  theme_bw()+labs(title = "Balance Histogram")+ theme(plot.title = element_text(size=12,hjust = 0.5,face = "bold"))

```
As we can see, $Balance$ is not normal, and has a right tail.

trying to predict the Balance, one cal ask about race and/or gender bias. Does this factor alone predict the result? and age?

```{r visual trend, error=FALSE, message=FALSE, warning=FALSE,cache=FALSE}
Credit %>%
  ggplot(aes(y=Balance, x=Age,fill= factor(Ethnicity) ))+
  geom_point(size= 0.7)+
  geom_smooth(method = "glm", level= 0.9)+ylim(min(Balance), max(1500))+
  labs(title = "Balance GLM by Age & Ethnicity")+ theme(plot.title = element_text(size=12,hjust = 0.5,face = "bold"))


Credit %>%
  ggplot(aes(y=Balance, x=Ethnicity,fill= Ethnicity))+
  geom_boxplot(size= 0.7)+labs(x = "")
```

and what if we will predict by race & gender?

```{r plot age race,cache=FALSE}
Credit %>%
  ggplot(aes(y=Balance,x= factor(Ethnicity),fill=factor(Gender) ))+
  geom_point(size= 0.7)+
  geom_boxplot(size= 0.7)+scale_fill_brewer(palette="Dark2")+
  labs(x= "", title = "Balance by Gender & Ethnicity")+theme(plot.title = element_text(size=12,hjust = 0.5,face = "bold"))
```
Again, there is no clear effect.

One can ask, can all of the weak predictors create a good one together?

So let's start!

We will ad some variables.

1. High_deg= $T$ if got more than 12 years of education
2. Age_2= $Age^2$ in order to allaw parabolic age effect
3. Bride= interaction of gender ans marriage

also, we will set seed and sample train & test.

```{r mutate, error=FALSE, message=FALSE, warning=FALSE,cache=FALSE}
Credit <- na.omit(Credit)
n <- nrow(Credit)
set.seed (13)
ntest <- trunc(n / 3)
testid <- sample (1:n, ntest)

Credit<- Credit[,-1] %>%
  mutate(High_deg= Education>=13)%>% 
  mutate(Age_2= Age^2)%>%
  mutate(Bride= (Gender== 'Female')&(Married== 'Yes'))
```

Finally, we can predict with our models

# Models

## Linear

### Classic LM

Assumption: knowing X matrix, Y distributed normal:
$(Y|X) \sim N(BX,\sigma^2)$

```{r LM, warning= F,cache=FALSE}
lmfit <- lm(Balance ~ ., data = Credit[-testid , ])
lmpred <- predict(lmfit , Credit[testid , ])
err_lm<- abs(Credit$Balance[testid] - lmpred)
mean(err_lm)
```

Now, what is our best predictors, if we filter our variables and prevent over fitting?

### step wise method

Adding each time variable according to AIC method.

```{r step wise ,cache=FALSE}
null_model <- lm(Balance ~1, data =  Credit[-testid , ]) #null model for starting Stepwise
step_fit <- stepAIC (null_model,k= 2, direction = "forward",scope = list(lower= formula(null_model),upper= formula(lmfit) ) )
step_pred <- predict(step_fit , Credit[testid , ])
err_step<- abs(Credit$Balance[testid] - step_pred)
mean(abs(err_step))
```
the chosen model is $$Balance ~ Rating + Income + Student + Limit + Cards$$ and we get sd of 80.94

lets see the regression vs the step wise. it is clear that the AIC method choose only the variables with P-value < 5 %

```{r sum of reg}
tab_model(lmfit,step_fit, show.ci= F,show.se = T,show.loglik= T)
```

### lasso

Assumption

$(Y|X) \sim N(BX,\sigma^2)$ , like LM.

But this time we use shrinkage method in order to reduce variance & over fitting. so our minimizing function define as

$RSS+ \lambda {\Sigma}_{j=1}^p |\beta_j|$

when 
$p=length( \beta)$ and $\lambda$
is a hyper parameter.

```{r Lasso}
x <- scale(model.matrix(Balance ~ . - 1, data = Credit))
y <- Credit$Balance
#glmnet
cvfit <- cv.glmnet(x[-testid , ], y[-testid], type.measure = "mae")
```

This time, we need to set our hipper parameter, 
$\lambda$
that lead to the minimum mean cross-validated error*

 <font size="2"> *[see also](https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet.pdf) </font> 
 
```{r lambda.min}
lambda.min<- cvfit$lambda.min
cpred <- predict(cvfit, x[testid , ], s = "lambda.min")
err_cp<- abs(y[testid] - cpred)
mean(abs(err_cp))
```
##  Trees

The main algorithm in random forest, adaboost, etc is splitting the data each time into two samples, in the most effective way by reevaluating the error function.

Mathematically, the tree assume a model of form

$f(x)= \sum^M_{m=1} c_m*I(x \in R_m) +\epsilon$

while $M$ is the numbers of groups, $R_m$ is the specific group & $c_m$ is the parameter of the model.

For a small tree,this is a very weak learner, but it can be used to create deeper learning. A complicated tree can lead to over-feeting.

###  Random Forest

A mean of n-tree
```{r randomForest, cache= T}
rf_s<- randomForest(formula= Balance ~ ., data = Credit[-testid , ],
                    ntree= 1500, mtry= 6, na.action = na.omit )

pred_s<- predict(rf_s,newdata = Credit[-testid , ] )
rf_pred <- predict(rf_s , Credit[testid , ], s = "lambda.min")
err_rf<- abs(y[testid] - rf_pred)
mean(abs(err_rf))
```

###  Adaboost

A mean of n-tree with weights
```{r adaboost, warning= F}
norm_y<- 2*y/(max(y)+min(y))-1

adb_s<-randomForest(formula= Balance ~ ., data = Credit[-testid , ],
                    ntree= 1500, mtry= 14, na.action = na.omit, importance= T )

pred_abs <- predict(adb_s , Credit[testid , ], s = "lambda.min")
pred_abs<- head(as.vector(pred_abs),133)
err_adb<- abs(y[testid] - pred_abs)
mean(abs(err_adb))
```
##  Neural Network

Creating of network of nonlinear function and weights, that evaluate the prediction.

This method is the hardest to present due to the complexitivity of the net.

###  set seed

we set our net using 2 layers of relu and then a dropout

```{r keras set, warning=FALSE}
modnn <- keras_model_sequential () %>%
  layer_dense(units = 50, activation = "relu",
              input_shape = ncol(x)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 20, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1)

modnn %>% compile(loss = "mse",
                  optimizer = optimizer_rmsprop (),
                  metrics = list("mean_absolute_error")
                  )
modnn
```

Using the net:

```{r apply Keras, echo=TRUE, results='hide', cache=TRUE}
mod_Credit <- modnn %>% fit(
  x[-testid , ], y[-testid], epochs = 1500, batch_size = 32,
  validation_data = list(x[testid , ], y[testid ])
)
```

### Keras result

```{r Keras result, cache=T}
#show Keras result
summary(mod_Credit)

plot(mod_Credit)+theme_gray()+
  theme(plot.caption = element_text(size = 6,hjust= 0),
        legend.position = c(.95, .95),
    legend.justification = c("right", "top"),
    legend.box.just = "right")#,
    #legend.margin = margin(6, 6, 6, 6))

nnpred <- predict(modnn , x[testid , ])
err_nn<- abs(y[testid] - nnpred)
mean(abs(err_nn))
```


## Sum all result

We used the same seed to test all methods, so now we can compare the error of each data.

* Each time this scipt were running, we got different result, due to randomness of $testid$ , and of the deep learning models

```{r sum pred, echo=TRUE, cache=T}
my_pred<- data.frame(c(lmpred,cpred,rf_pred,pred_abs,nnpred)) #pred data frame
m<- length(testid)
modl_nam<-c("Linear", "Lasso", "Random_forrest","Adaboost","Neural_network")
my_pred$Model<- c(rep(modl_nam[1],m),rep(modl_nam[2],m),rep(modl_nam[3],m),
                  rep(modl_nam[4],m),rep(modl_nam[5],m) ) #naming each model in pred data frame
my_pred$Balance<-rep(y[testid],5)
colnames(my_pred)[1]<- "Predict"

my_err<- as.data.frame(cbind(err_lm,err_cp,err_rf,err_adb,err_nn))
my_var<- colMeans(my_err[sapply(my_err, is.numeric)]) 

#caption- var of the models alphabeticaly
order_script<- order(modl_nam)
script<- paste("sd of models: ",modl_nam[order_script[1]], "is", round(my_var[order_script[1]],3))
for (i in order_script[-1]) {
  script<- paste(script, ", ", modl_nam[i], "is", round(my_var[i],3) )}

my_pred %>%
  ggplot( aes(x= Model ,y=Balance-Predict, fill= Model)) +
  geom_violin()+
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color="black", size=0.45) +
  theme_dark() +
  theme(
    legend.position="none",
    plot.title = element_text(size=12,hjust = 0.5,face = "bold"),
    plot.caption = element_text(size = 8,hjust= 0),
    axis.title = element_text(size = 8))+ ylab("Error")+
  labs(title = "Error Violin",caption = script)
```
# Discussion

If the winner of the error in prediction were not that clear, our $H_0$ would will be the linear model, since it's prediction is the easiest to understand and present.

Amazingly, <b>the LM(var 82) did nothing compare to the NN(var 22)</b>, with only almost quarter of the linear's error.

At the same time, Lasso(var 81) did only slightly better then the liner, both better than the ADB(var 85).

Another clear effect is this of the ADB comparing to the RF(var 101), which had the worse prediction variance. The effect if weights is the main advantage of ADB over RF.

To sum it up, though rerunning of this script might create a different result, this modeling comparing to data frame prediction show us how <b>using weights or regulation in models can get better models comparing to the same model. In contrast, some time different models create worse prediction than the unregulated ones</b>, like RF and LM. The most complex model, NN, overcome all models,
and known to have huge potential, as long as understanding the effect of each variable is not needed.

<b>In my opinion, the best models to predict with here are the Lasso and the Neural network.</b>

.